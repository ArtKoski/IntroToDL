A dropout is a regularization technique, where neurons of a layer are dropped from calculations at random. Since it's random, in practice, even very dominant neurons can be dropped out every now and then, reducing overfitting.

Dropout layers can and are often placed after dense layers.  