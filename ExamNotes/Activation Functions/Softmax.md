Softmax is an activation function that maps a numeric vector into a probability distribution, i.e., the output values sum up to one. It is used often in the last layer of a neural network, so that the networks results can be interpreted as probabilities for each class. The output can then be dictated from the output with the highest probability.