When training a neural network, the training data is divided into small batches.

One reason for this is efficiency, since training all the data at once could be very memory-intensive. Many small batches and their computations can also be parallelized. 

Another reason is that it results in better generalization performance.