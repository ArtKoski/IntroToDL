{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torch.utils.data\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch.backends.mps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- hyperparameters ---\n",
    "N_EPOCHS = 10\n",
    "LR = 0.001\n",
    "DROPOUT_RATE = 0.3\n",
    "\n",
    "#--- fixed constants ---\n",
    "NUM_CLASSES = 14\n",
    "DATA_DIR = 'dataset/%s'\n",
    "NUM_CHANNELS = 3\n",
    "WIDTH, HEIGHT = 128, 128\n",
    "MEAN = [0.45183619, 0.4171191, 0.3778775 ]\n",
    "SD = [0.24016619, 0.23024313, 0.22750713]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "                                        transforms.RandomHorizontalFlip(),\n",
    "                                        transforms.RandomResizedCrop(128),\n",
    "                                        #transforms.Grayscale(),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize(MEAN, SD)])\n",
    "val_transform = transforms.Compose([\n",
    "                                        #transforms.Grayscale(),\n",
    "                                        transforms.Resize(256),\n",
    "                                        transforms.CenterCrop(128),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize(MEAN, SD)])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize(MEAN, SD)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = sorted(os.listdir(os.path.join(self.root_dir, \"images\")))\n",
    "        self.labels = {}\n",
    "        for label in ['baby', 'bird', 'car', 'clouds', 'dog', 'female', 'flower', 'male', 'night', 'people', 'portrait', 'river', 'sea', 'tree']:\n",
    "            with open(os.path.join(self.root_dir, \"annotations\", f\"{label}.txt\"), \"r\") as f:\n",
    "                f.seek(0)\n",
    "                self.labels[label] = sorted(set(f.read().splitlines()))\n",
    "                # self.labels[label] = set(f.read().splitlines()) # this is original\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = os.path.join(self.root_dir, \"images\", self.image_paths[index])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = []\n",
    "        for key, value in self.labels.items():\n",
    "            filename = os.path.basename(image_path)[2:-4]\n",
    "            # print(filename)\n",
    "            label.append(1 if filename in value else 0)\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "        return image, label\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set here the path where the images and annotations are\n",
    "path = \"/home/vixmaria/koulu/IDL/projekti/IntroToDL-main/dataset\"\n",
    "\n",
    "# load the data\n",
    "dataset = CustomDataset(path, transform=None)\n",
    "\n",
    "# Define the sizes of the splits\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "# Split the dataset randomly into train, validation, and test sets\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_dataset.dataset = CustomDataset(path, transform=train_transform)\n",
    "val_dataset.dataset = CustomDataset(path, transform=val_transform)\n",
    "test_dataset.dataset = CustomDataset(path, transform=test_transform)\n",
    "\n",
    "\n",
    "# Create data loaders for each set\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiLabelClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(NUM_CHANNELS, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * 32 * 32, 128)\n",
    "        self.fc2 = nn.Linear(128, NUM_CLASSES)\n",
    "        self.drop = nn.Dropout(p=DROPOUT_RATE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 32 * 32)\n",
    "        x = self.drop(x)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Epoch: 01\n",
      "\tTrain Loss: 0.815 | Train Acc: 89.68%\n",
      "\t Val. Loss: 0.733 |  Val. Acc: 92.89%\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.712 | Train Acc: 92.76%\n",
      "\t Val. Loss: 0.705 |  Val. Acc: 92.89%\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.700 | Train Acc: 92.76%\n",
      "\t Val. Loss: 0.699 |  Val. Acc: 92.89%\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.697 | Train Acc: 92.76%\n",
      "\t Val. Loss: 0.697 |  Val. Acc: 92.89%\n",
      "Epoch: 05\n",
      "\tTrain Loss: 0.696 | Train Acc: 92.76%\n",
      "\t Val. Loss: 0.696 |  Val. Acc: 92.89%\n",
      "Epoch: 06\n",
      "\tTrain Loss: 0.695 | Train Acc: 92.76%\n",
      "\t Val. Loss: 0.695 |  Val. Acc: 92.89%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 50\u001b[0m\n\u001b[1;32m     48\u001b[0m     loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     49\u001b[0m     train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n\u001b[0;32m---> 50\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     51\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     53\u001b[0m     \u001b[39m# print('predictions',predictions)\u001b[39;00m\n\u001b[1;32m     54\u001b[0m     \u001b[39m# print('labels',labels)\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[39m# calculate average loss and accuracy for epoch\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = MultiLabelClassifier()\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# set up device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    if torch.backends.mps.is_built():\n",
    "        device = torch.device('mps')\n",
    "\n",
    "\n",
    "print(device)\n",
    "\n",
    "model = MultiLabelClassifier().to(device)\n",
    "\n",
    "# Define your loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LR)\n",
    "threshold = 0.5\n",
    "\n",
    "# parameters for early stop\n",
    "threshold = 1       # number of epochs to wait if test loss doesnt improve\n",
    "min_loss = np.inf\n",
    "counter = 0\n",
    "\n",
    "# Train your model\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0.0\n",
    "    train_total = 0\n",
    "\n",
    "    model.train()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # calculate accuracy\n",
    "        predictions = (outputs > 0.5).int()\n",
    "        correct = torch.eq(predictions, labels).sum().item()\n",
    "\n",
    "        train_correct += correct\n",
    "\n",
    "        # En oo iha varma miten tätä pitäs tulkita, atm laskee siis jokasen oikeen arvauksen ja yhen kuvan sisällä on käytännössä 14 arvausta\n",
    "        train_total += labels.size(0)*NUM_CLASSES\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        train_loss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print('predictions',predictions)\n",
    "        # print('labels',labels)\n",
    "    \n",
    "    # calculate average loss and accuracy for epoch\n",
    "    epoch_loss = train_loss / len(train_loader)\n",
    "    epoch_accuracy = train_correct / train_total\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {epoch_loss:.3f} | Train Acc: {epoch_accuracy*100:.2f}%')\n",
    "\n",
    "    # evaluate model on validation set\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        for i, data in enumerate(val_loader):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model.forward(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # calculate accuracy\n",
    "            predictions = (outputs > 0.5).int()\n",
    "            correct = torch.eq(predictions, labels).sum().item()\n",
    "            val_correct += correct\n",
    "\n",
    "            val_total += labels.size(0)*NUM_CLASSES\n",
    "            val_loss += loss\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = val_correct / val_total\n",
    "\n",
    "    print(f'\\t Val. Loss: {val_loss:.3f} |  Val. Acc: {val_acc*100:.2f}%')\n",
    "\n",
    "    # early stopping\n",
    "    if val_loss < min_loss:\n",
    "        min_loss = val_loss\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= threshold:\n",
    "            print(f'Stopping early after epoch {epoch}, best loss reached.')\n",
    "            break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.694 |  Test Acc: 92.79%\n"
     ]
    }
   ],
   "source": [
    "#--- test ---\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_loss = 0\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    for i, data in enumerate(test_loader):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model.forward(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # calculate accuracy\n",
    "        predictions = (outputs > 0.5).int()\n",
    "        \n",
    "        correct = torch.eq(predictions, labels).sum().item()\n",
    "        test_correct += correct\n",
    "\n",
    "        test_total += labels.size(0)*NUM_CLASSES\n",
    "        test_loss += loss\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    test_acc = test_correct / test_total\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9ce84d8e4502ddaac3896d19b3e19296246c03597ea522fc771d0c9d54b494e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
